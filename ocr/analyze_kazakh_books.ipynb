{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./corpus/leipzig.csv CSV file in chunks 4166666...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:15, 15.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to output file...\n",
      "Processing ./corpus/oscar.csv CSV file in chunks 116009...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [02:18, 46.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to output file...\n",
      "Processing ./corpus/kazakhBooks.csv CSV file in chunks 1241...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [03:39, 31.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to output file...\n",
      "Processing ./corpus/cc100-monolingual-crawled-data.csv CSV file in chunks 4166666...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [04:34, 54.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to output file...\n",
      "Processing ./corpus/kazakhNews.csv CSV file in chunks 239234...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [09:27, 40.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to output file...\n",
      "Writing to total output file...\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import string\n",
    "\n",
    "def get_word_pattern():\n",
    "    numbers = \"0123456789\"\n",
    "    other_symbols = string.punctuation + \"«»…№°—\"\n",
    "    space_symbol = ''\n",
    "    kazakh_letters = 'АБВГДЕЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯЁабвгдежзийклмнопрстуфхцчшщъыьэюяёӘҒҚҢӨҰҮІҺәғқңөұүіһ'\n",
    "    english_letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'\n",
    "    all_letters = kazakh_letters + english_letters\n",
    "    all_symbols = numbers + other_symbols + space_symbol + all_letters\n",
    "    all_symbols = re.escape(all_symbols)\n",
    "\n",
    "    # before 8885443 words\n",
    "    \n",
    "    valid_chars = f'[{all_symbols}]'\n",
    "    return re.compile(f'{valid_chars}+')\n",
    "\n",
    "def process_chunk(chunk, word_counts, word_pattern):\n",
    "    for text in chunk['text']:\n",
    "        words = word_pattern.findall(str(text))\n",
    "        word_counts.update(words)\n",
    "\n",
    "def get_word_frequencies(in_file, chunk_size=1000):\n",
    "    word_counts = collections.Counter()\n",
    "    word_pattern = get_word_pattern()\n",
    "\n",
    "    print(f'Processing {in_file} CSV file in chunks {chunk_size}...')\n",
    "    for chunk in tqdm(pd.read_csv(in_file, chunksize=chunk_size, usecols=['text', 'contains_kaz_symbols'])):\n",
    "        process_chunk(chunk, word_counts, word_pattern)\n",
    "\n",
    "    return word_counts\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "    Dataset Split\tDomain\tNumber of texts in Split\tNumber of tokens in Split\tNumber of unique tokens in Split\tMedian number of tokens in text\n",
    "cc100-monolingual-crawled-data\tWikipedia articles\t19 635 580\t441 623 321\t6 217 337\t12\n",
    "kazakhBooks\tBooks\t8 423\t351 433 586\t7 245 720\t40 264\n",
    "leipzig\tArticles/News\t1 706 485\t26 494 864\t1 109 113\t14\n",
    "oscar\tCommonCrawl\t269 047\t230 314 378\t3 863 498\t431\n",
    "kazakhNews\tNews\t3 264 273\t1 041 698 037\t5 820 543\t209\n",
    "    \"\"\"\n",
    "    tokens_per_chunk = 50000000\n",
    "    original_inputs = [\n",
    "        ('leipzig', 12),\n",
    "        ('oscar', 431),\n",
    "        ('kazakhBooks', 40264),\n",
    "        ('cc100-monolingual-crawled-data', 12),\n",
    "        ('kazakhNews', 209),\n",
    "    ]\n",
    "    total_counter = collections.Counter()\n",
    "    for in_file, median_tokens in original_inputs:\n",
    "        chunk_size = tokens_per_chunk // median_tokens\n",
    "        word_counts = get_word_frequencies(f'./corpus/{in_file}.csv', chunk_size=chunk_size)\n",
    "        total_counter = total_counter + word_counts\n",
    "\n",
    "        print('Writing to output file...')\n",
    "        with open(f'./corpus_output/{in_file}_words.csv', 'w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.writer(outfile, delimiter='\\t')\n",
    "            writer.writerow(['word', 'count'])\n",
    "            writer.writerows((word, count) for word, count in word_counts.most_common() if count >= 2)\n",
    "\n",
    "    print('Writing to total output file...')\n",
    "    with open(f'./corpus_output/total_words.csv', 'w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.writer(outfile, delimiter='\\t')\n",
    "        writer.writerow(['word', 'count'])\n",
    "        writer.writerows((word, count) for word, count in total_counter.most_common() if count >= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5431985/5431985 [03:12<00:00, 28158.78it/s]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv('./corpus_output/total_words.csv', sep='\\t')\n",
    "df = df[df['count'] >= 5]\n",
    "df['word_len'] = df['word'].apply(str).apply(len)\n",
    "df = df[df['word_len'] < 26]\n",
    "df = df.drop(columns=['word_len'])\n",
    "\n",
    "\n",
    "# Latin to Cyrillic character mapping (substituting similar-looking characters)\n",
    "latin_to_cyrillic = {\n",
    "    'a': 'а',  # Latin 'a' -> Cyrillic 'а'\n",
    "    'A': 'А',  # Latin 'A' -> Cyrillic 'А'\n",
    "    'e': 'е',  # Latin 'e' -> Cyrillic 'е'\n",
    "    'E': 'Е',  # Latin 'E' -> Cyrillic 'Е'\n",
    "    'i': 'і',  # Latin 'i' -> Cyrillic 'і'\n",
    "    'I': 'І',  # Latin 'I' -> Cyrillic 'І'\n",
    "    'o': 'о',  # Latin 'o' -> Cyrillic 'о'\n",
    "    'O': 'О',  # Latin 'O' -> Cyrillic 'О'\n",
    "    'p': 'р',  # Latin 'p' -> Cyrillic 'р'\n",
    "    'P': 'Р',  # Latin 'P' -> Cyrillic 'Р'\n",
    "    'c': 'с',  # Latin 'c' -> Cyrillic 'с'\n",
    "    'C': 'С',  # Latin 'C' -> Cyrillic 'С'\n",
    "    'y': 'у',  # Latin 'y' -> Cyrillic 'у'\n",
    "    'Y': 'У',  # Latin 'Y' -> Cyrillic 'У'\n",
    "    'x': 'х',   # Latin 'x' -> Cyrillic 'х'\n",
    "    'X': 'Х',   # Latin 'X' -> Cyrillic 'Х'\n",
    "    'H': 'Н',  # Latin 'H' -> Cyrillic 'Н'\n",
    "    'K': 'К',  # Latin 'K' -> Cyrillic 'К'\n",
    "    'M': 'М',  # Latin 'M' -> Cyrillic 'М'\n",
    "    'T': 'Т',  # Latin 'T' -> Cyrillic 'Т'\n",
    "    'B': 'В',  # Latin 'B' -> Cyrillic 'В'\n",
    "}\n",
    "\n",
    "# Reverse mapping: Cyrillic to Latin (to handle the inverse direction)\n",
    "cyrillic_to_latin = {v: k for k, v in latin_to_cyrillic.items()}\n",
    "\n",
    "trans_table = str.maketrans(latin_to_cyrillic)\n",
    "\n",
    "# Normalize a word to a standard Latin form by handling both Latin->Cyrillic and Cyrillic->Latin transformations\n",
    "def normalize_word(word):\n",
    "    return  word.translate(trans_table)\n",
    "\n",
    "# Read the CSV data\n",
    "data = df\n",
    "\n",
    "# convert word to string\n",
    "data['word'] = data['word'].astype(str)\n",
    "\n",
    "# size before\n",
    "print(data.shape)\n",
    "\n",
    "# Create a dictionary to store the most frequent word for each normalized form\n",
    "normalized_dict = {}\n",
    "\n",
    "# Process each word and count the most frequent occurrence of similar-looking words\n",
    "for index, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "    word = row['word']\n",
    "    count = row['count']\n",
    "    \n",
    "    # Normalize the word\n",
    "    normalized_word = normalize_word(word)\n",
    "    \n",
    "    # If the normalized word is already in the dictionary, compare counts\n",
    "    if normalized_word in normalized_dict:\n",
    "        # Keep the most frequent one\n",
    "        if normalized_dict[normalized_word][1] < count:\n",
    "            normalized_dict[normalized_word] = (word, count)\n",
    "    else:\n",
    "        normalized_dict[normalized_word] = (word, count)\n",
    "\n",
    "# Create a final list of words with the highest frequency in each group\n",
    "final_words = [(word, count) for word, count in normalized_dict.values()]\n",
    "\n",
    "# Sort by count (highest first)\n",
    "final_words.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Convert the result to a DataFrame\n",
    "final_df = pd.DataFrame(final_words, columns=['word', 'count'])\n",
    "\n",
    "# size after\n",
    "print(final_df.shape)\n",
    "\n",
    "# Save or display the final output\n",
    "final_df.to_csv('./corpus_output/cleaned_words.csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5303273/5303273 [02:44<00:00, 32165.15it/s]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv('./corpus_output/cleaned_words.csv', sep='\\t')\n",
    "df = df[df['count'] >= 5]\n",
    "df['word_len'] = df['word'].apply(str).apply(len)\n",
    "df = df[df['word_len'] < 26]\n",
    "df = df.drop(columns=['word_len'])\n",
    "\n",
    "# write only words to txt file, several times: loge(count / 10)\n",
    "with open('corpus_words.txt', 'w', encoding='utf-8') as f:\n",
    "    for index, row in tqdm(df.iterrows(), total = df.shape[0]):\n",
    "        word = str(row['word'])\n",
    "        for i in range(math.ceil(math.log(row['count'] / 100))):\n",
    "            f.write(word)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
