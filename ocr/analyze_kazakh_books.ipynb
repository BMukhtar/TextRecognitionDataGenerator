{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./corpus/leipzig.csv CSV file in chunks 4166666...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:13, 13.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to output file...\n",
      "Processing ./corpus/oscar.csv CSV file in chunks 116009...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [01:30, 30.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to output file...\n",
      "Processing ./corpus/kazakhBooks.csv CSV file in chunks 1241...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [02:00, 17.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to output file...\n",
      "Processing ./corpus/cc100-monolingual-crawled-data.csv CSV file in chunks 4166666...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [03:42, 44.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to output file...\n",
      "Processing ./corpus/kazakhNews.csv CSV file in chunks 239234...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [02:37, 11.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to output file...\n",
      "Writing to total output file...\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import string\n",
    "\n",
    "def get_word_pattern():\n",
    "    kazakh_letters = 'АБВГДЕЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯЁабвгдежзийклмнопрстуфхцчшщъыьэюяёӘҒҚҢӨҰҮІҺәғқңөұүіһ'\n",
    "    english_letters = 'a-zA-Z'\n",
    "    numbers = '0-9'\n",
    "    other_symbols = '»…£€¥¢฿₸₽№°—'\n",
    "    punctuation = re.escape(string.punctuation)\n",
    "    \n",
    "    valid_chars = f'[{kazakh_letters}{english_letters}{numbers}{other_symbols}{punctuation}]'\n",
    "    return re.compile(f'{valid_chars}+')\n",
    "\n",
    "def process_chunk(chunk, word_counts, word_pattern):\n",
    "    for text in chunk.loc[chunk['contains_kaz_symbols'] == 1, 'text']:\n",
    "        words = word_pattern.findall(str(text))\n",
    "        word_counts.update(words)\n",
    "\n",
    "def get_word_frequencies(in_file, chunk_size=1000):\n",
    "    word_counts = collections.Counter()\n",
    "    word_pattern = get_word_pattern()\n",
    "\n",
    "    print(f'Processing {in_file} CSV file in chunks {chunk_size}...')\n",
    "    for chunk in tqdm(pd.read_csv(in_file, chunksize=chunk_size, usecols=['text', 'contains_kaz_symbols'])):\n",
    "        process_chunk(chunk, word_counts, word_pattern)\n",
    "\n",
    "    return word_counts\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "    Dataset Split\tDomain\tNumber of texts in Split\tNumber of tokens in Split\tNumber of unique tokens in Split\tMedian number of tokens in text\n",
    "cc100-monolingual-crawled-data\tWikipedia articles\t19 635 580\t441 623 321\t6 217 337\t12\n",
    "kazakhBooks\tBooks\t8 423\t351 433 586\t7 245 720\t40 264\n",
    "leipzig\tArticles/News\t1 706 485\t26 494 864\t1 109 113\t14\n",
    "oscar\tCommonCrawl\t269 047\t230 314 378\t3 863 498\t431\n",
    "kazakhNews\tNews\t3 264 273\t1 041 698 037\t5 820 543\t209\n",
    "    \"\"\"\n",
    "    tokens_per_chunk = 50000000\n",
    "    original_inputs = [\n",
    "        ('leipzig', 12),\n",
    "        ('oscar', 431),\n",
    "        ('kazakhBooks', 40264),\n",
    "        ('cc100-monolingual-crawled-data', 12),\n",
    "        ('kazakhNews', 209),\n",
    "    ]\n",
    "    total_counter = collections.Counter()\n",
    "    for in_file, median_tokens in original_inputs:\n",
    "        chunk_size = tokens_per_chunk // median_tokens\n",
    "        word_counts = get_word_frequencies(f'./corpus/{in_file}.csv', chunk_size=chunk_size)\n",
    "        total_counter = total_counter + word_counts\n",
    "\n",
    "        print('Writing to output file...')\n",
    "        with open(f'./corpus_output/{in_file}_words.csv', 'w', newline='', encoding='utf-8') as outfile:\n",
    "            writer = csv.writer(outfile, delimiter='\\t')\n",
    "            writer.writerow(['word', 'count'])\n",
    "            writer.writerows((word, count) for word, count in word_counts.most_common() if count >= 2)\n",
    "\n",
    "    print('Writing to total output file...')\n",
    "    with open(f'./corpus_output/total_words.csv', 'w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.writer(outfile, delimiter='\\t')\n",
    "        writer.writerow(['word', 'count'])\n",
    "        writer.writerows((word, count) for word, count in total_counter.most_common() if count >= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8842087/8842087 [04:37<00:00, 31821.01it/s]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "df = pd.read_csv('./corpus_output/total_words.csv', sep='\\t')\n",
    "df = df[df['count'] >= 2]\n",
    "df['word_len'] = df['word'].apply(str).apply(len)\n",
    "df = df[df['word_len'] < 26]\n",
    "\n",
    "# write only words to txt file, several times: loge(count / 10)\n",
    "with open('corpus_words.txt', 'w', encoding='utf-8') as f:\n",
    "    for index, row in tqdm(df.iterrows(), total = df.shape[0]):\n",
    "        word = str(row['word'])\n",
    "        for i in range(math.ceil(math.log(row['count'] / 10))):\n",
    "            f.write(word)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
